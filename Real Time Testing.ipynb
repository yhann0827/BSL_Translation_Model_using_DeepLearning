{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eafeefd",
   "metadata": {},
   "source": [
    "## Real-Time Testing Code\n",
    "Run the following code to evaluate the model in real-time. When activated, the model will identify the gestures and display the corresponding text on the screen upon recognition. Users have the option to test the model using any of the 15 gestures demonstrated in the tutorial video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8891a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SetUp\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11bd5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    image.flags.writeable = False                  \n",
    "    results = model.process(image)                \n",
    "    image.flags.writeable = True                   \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a2adb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 ) \n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(0, 128, 0), thickness=2, circle_radius=4), \n",
    "                                 mp_drawing.DrawingSpec(color=(128, 0, 128), thickness=2, circle_radius=2)\n",
    "                                 ) \n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(0, 0, 128), thickness=2, circle_radius=4), \n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 ) \n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(0, 0, 128), thickness=2, circle_radius=4), \n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af1bb29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    if results.pose_landmarks:  \n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten()\n",
    "    else:  \n",
    "        pose = np.zeros(33*4)\n",
    "\n",
    "    if results.face_landmarks:  \n",
    "        face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten()\n",
    "    else: \n",
    "        face = np.zeros(468*3)\n",
    "\n",
    "    if results.left_hand_landmarks:  \n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten()\n",
    "    else:  \n",
    "        lh = np.zeros(21*3)\n",
    "\n",
    "    if results.right_hand_landmarks:  \n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten()\n",
    "    else: \n",
    "        rh = np.zeros(21*3)\n",
    "\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61bd5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_actions = ['Hello', 'Goodbye', 'Please', 'Sorry', 'Welcome']\n",
    "basic_actions = ['Maybe', 'Family', 'Should', 'Yes', 'Sure']\n",
    "question_actions = ['Ask','Why', 'What', 'When', 'How']\n",
    "\n",
    "actions = greeting_actions + basic_actions + question_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fac70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "video = []\n",
    "sentence = []\n",
    "threshold = 0.8\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Users can test one of the three models by removing the comment symbol from the chosen model's code. \n",
    "model = tf.keras.models.load_model('grubslmodel.keras')\n",
    "# model = tf.keras.models.load_model('simplernn.keras')\n",
    "# model = tf.keras.models.load_model('lstmmodel.keras')\n",
    "with mp_holistic.Holistic (\n",
    "    min_detection_confidence=0.5, \n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        video.insert(0,keypoints)\n",
    "        video = video[:30]\n",
    "        if len(video) == 30:\n",
    "            res = model.predict(np.expand_dims(video, axis=0))[0]\n",
    "            print('Detected Word:' + actions[np.argmax(res)])\n",
    "            if res[np.argmax(res)] > threshold:\n",
    "                predicted_action = actions[np.argmax(res)]\n",
    "                if len(sentence) <= 0 or predicted_action != sentence[-1]:\n",
    "                    sentence.append(predicted_action)\n",
    "            if len(sentence) > 1: \n",
    "                sentence = sentence[-1:]\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (200, 200, 200), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_DUPLEX, 0.8, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Real-Time Testing', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f250694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
